### 简述
该文章是利用神经网络（GPT）完成文本压缩。文本压缩的方法是算术编码，利用GPT预测每个文本的分布概率，或者将分布概率从大到小排序。总体上，还是使用的传统压缩方法，使用神经网络完成分布概率的计算，该领域也称为**神经网络压缩**。


### 文章具体贡献
1. 使用GPT定义了压缩长度 $\sum_{i=1} log_2(P_i(t_i))$

![image](https://github.com/AlbertLin0/ArxivSchedulePaper/assets/58505644/3f877d5b-eea0-41b2-aea4-08b4386eecf2)
2. 算数编码中，有个强假设是概率密度已知，GPT用来预测概率密度。上图是编码过程，Decoder是传统的算数编码。

### 衡量信息距离，即文本相似度方法
Definition 1: $K(x) \approx C(x) =\sum_{i=1} P_i(x_i)$

Definition 2: $K(x/y) \approx C(x|y) = \sum_{i=1} q_i(x_i), q_i(x_i) = \phi (y,x_{1:i-1})$，第二个定义式为使用GPT，输入句子y 和前i个x句子中的token，得到的概率密度函数

x,y文本的相似性M为：


```math
M_{max}(x,y) = \frac{max{K(x|y), K(y|x)}}{max{K(x),K(y)}}
```
```math
M_{min}(x,y) = \frac{min{K(x|y), K(y|x)}}{min{K(x),K(y)}}
```


###### 特点
零样本，不需要微调或提示工程
###### 为什么可行
GPT在本质上是概率模型，根据上文，预测文本的概率是可行的。
###### 缺点
显然，预测概率需要时间，虽然压缩比很高，但需要的时间更多

### 总结
1. 利用LLM进行无损压缩优势很大，但通篇没说到时间花费
2. 因为时间花费，且很难降低；最优秀的压缩算法肯定是近似每个token的分布概率，简单的算法则使用贪心作概率密度假设。
3. 该文章为挤牙膏，只可实验，不能实用。

### 术语
- Kolmogorov complexity：该复杂度不可计算，通常用来探讨对象的内在复杂度和压缩的极限。
- 压缩长度：对信息压缩后的结果长度，该文章定义为 $\sum_{i=1} log_2(P_i(t_i))$，t为文本词，P为概率，P(t)，t文本的概率密度
- Universal information Distance：即文本相似度度量
